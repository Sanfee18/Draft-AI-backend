{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab554c6",
   "metadata": {},
   "source": [
    "# Deploy Stable Diffusion on a SageMaker GPU Multi-Model Endpoint with Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5640df9b",
   "metadata": {},
   "source": [
    "In this notebook we will deploy multiple variations of Stable Diffusion on a SageMaker Multi-Model GPU Endpoint (MME GPU) powered by NVIDIA Triton Inference Server.\n",
    "> ⚠ **Warning**: This notebook requires a minimum of an `ml.m5.large` instance to build the conda environment required for hosting the Stable Diffusion models.  \n",
    "\n",
    "Skip to:\n",
    "1. [Installs and imports](#installs)\n",
    "2. [Download pretrained model](#modelartifact)\n",
    "3. [Packaging a conda environment](#condaenv)\n",
    "4. [Deploy to SageMaker Real-Time Endpoint](#deploy)\n",
    "6. [Query Models](#query)\n",
    "7. [Clean up](#cleanup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf35ff",
   "metadata": {},
   "source": [
    "### Part 1 - Installs and imports <a name=\"installs\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69df6cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker\n",
      "  Using cached sagemaker-2.227.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pillow\n",
      "  Downloading pillow-10.4.0-cp38-cp38-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting huggingface-hub\n",
      "  Using cached huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting conda-pack\n",
      "  Using cached conda_pack-0.8.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting attrs<24,>=23.1.0 (from sagemaker)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting boto3<2.0,>=1.34.142 (from sagemaker)\n",
      "  Downloading boto3-1.34.155-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting cloudpickle==2.2.1 (from sagemaker)\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting google-pasta (from sagemaker)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting numpy<2.0,>=1.9.0 (from sagemaker)\n",
      "  Using cached numpy-1.24.4-cp38-cp38-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Collecting protobuf<5.0,>=3.12 (from sagemaker)\n",
      "  Using cached protobuf-4.25.4-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Collecting smdebug-rulesconfig==1.0.1 (from sagemaker)\n",
      "  Using cached smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting importlib-metadata<7.0,>=1.4.0 (from sagemaker)\n",
      "  Using cached importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.8/site-packages (from sagemaker) (24.1)\n",
      "Collecting pandas (from sagemaker)\n",
      "  Using cached pandas-2.0.3-cp38-cp38-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting pathos (from sagemaker)\n",
      "  Using cached pathos-0.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting schema (from sagemaker)\n",
      "  Using cached schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
      "Collecting PyYAML~=6.0 (from sagemaker)\n",
      "  Downloading pyyaml-6.0.2.tar.gz (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jsonschema (from sagemaker)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: platformdirs in ./.conda/lib/python3.8/site-packages (from sagemaker) (3.10.0)\n",
      "Collecting tblib<4,>=1.7.0 (from sagemaker)\n",
      "  Using cached tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting urllib3<3.0.0,>=1.26.8 (from sagemaker)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting requests (from sagemaker)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting docker (from sagemaker)\n",
      "  Using cached docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting tqdm (from sagemaker)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in ./.conda/lib/python3.8/site-packages (from sagemaker) (5.9.0)\n",
      "Collecting filelock (from huggingface-hub)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/lib/python3.8/site-packages (from huggingface-hub) (4.11.0)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.8/site-packages (from conda-pack) (72.1.0)\n",
      "Collecting botocore<1.35.0,>=1.34.155 (from boto3<2.0,>=1.34.142->sagemaker)\n",
      "  Downloading botocore-1.34.155-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0,>=1.34.142->sagemaker)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0,>=1.34.142->sagemaker)\n",
      "  Using cached s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.conda/lib/python3.8/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.17.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->sagemaker)\n",
      "  Using cached charset_normalizer-3.3.2-cp38-cp38-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->sagemaker)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->sagemaker)\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: six in ./.conda/lib/python3.8/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Collecting importlib-resources>=1.4.0 (from jsonschema->sagemaker)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->sagemaker)\n",
      "  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pkgutil-resolve-name>=1.3.10 (from jsonschema->sagemaker)\n",
      "  Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl.metadata (624 bytes)\n",
      "Collecting referencing>=0.28.4 (from jsonschema->sagemaker)\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema->sagemaker)\n",
      "  Downloading rpds_py-0.20.0-cp38-cp38-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->sagemaker)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->sagemaker)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting ppft>=1.7.6.8 (from pathos->sagemaker)\n",
      "  Using cached ppft-1.7.6.8-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dill>=0.3.8 (from pathos->sagemaker)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pox>=0.3.4 (from pathos->sagemaker)\n",
      "  Using cached pox-0.3.4-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting multiprocess>=0.70.16 (from pathos->sagemaker)\n",
      "  Downloading multiprocess-0.70.16-py38-none-any.whl.metadata (7.1 kB)\n",
      "Collecting urllib3<3.0.0,>=1.26.8 (from sagemaker)\n",
      "  Downloading urllib3-1.26.19-py2.py3-none-any.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached sagemaker-2.227.0-py3-none-any.whl (1.5 MB)\n",
      "Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Using cached smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Downloading pillow-10.4.0-cp38-cp38-macosx_11_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "Using cached conda_pack-0.8.0-py2.py3-none-any.whl (33 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Downloading boto3-1.34.155-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Using cached numpy-1.24.4-cp38-cp38-macosx_11_0_arm64.whl (13.8 MB)\n",
      "Using cached protobuf-4.25.4-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Using cached tblib-3.0.0-py3-none-any.whl (12 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached pandas-2.0.3-cp38-cp38-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Using cached pathos-0.3.2-py3-none-any.whl (82 kB)\n",
      "Using cached schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
      "Downloading botocore-1.34.155-py3-none-any.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.19-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp38-cp38-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Downloading multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
      "Using cached pox-0.3.4-py3-none-any.whl (29 kB)\n",
      "Using cached ppft-1.7.6.8-py3-none-any.whl (56 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.20.0-cp38-cp38-macosx_11_0_arm64.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Building wheels for collected packages: PyYAML\n",
      "  Building wheel for PyYAML (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyYAML: filename=PyYAML-6.0.2-cp38-cp38-macosx_11_0_arm64.whl size=45364 sha256=0a5179d6c6e38f8f05fd05cf5c1bbceb103ce24987da37f86cc2d12963b5b27c\n",
      "  Stored in directory: /Users/davidsanfelix/Library/Caches/pip/wheels/ab/34/2b/5ad179dc5914ad34b84b05b4fd1c3f7a39fee3e771ddf534a4\n",
      "Successfully built PyYAML\n",
      "Installing collected packages: schema, pytz, urllib3, tzdata, tqdm, tblib, smdebug-rulesconfig, rpds-py, PyYAML, protobuf, ppft, pox, pkgutil-resolve-name, pillow, numpy, jmespath, importlib-resources, importlib-metadata, idna, google-pasta, fsspec, filelock, dill, conda-pack, cloudpickle, charset-normalizer, certifi, attrs, requests, referencing, pandas, multiprocess, botocore, s3transfer, pathos, jsonschema-specifications, huggingface-hub, docker, jsonschema, boto3, sagemaker\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 7.0.1\n",
      "    Uninstalling importlib-metadata-7.0.1:\n",
      "      Successfully uninstalled importlib-metadata-7.0.1\n",
      "Successfully installed PyYAML-6.0.2 attrs-23.2.0 boto3-1.34.155 botocore-1.34.155 certifi-2024.7.4 charset-normalizer-3.3.2 cloudpickle-2.2.1 conda-pack-0.8.0 dill-0.3.8 docker-7.1.0 filelock-3.15.4 fsspec-2024.6.1 google-pasta-0.2.0 huggingface-hub-0.24.5 idna-3.7 importlib-metadata-6.11.0 importlib-resources-6.4.0 jmespath-1.0.1 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 multiprocess-0.70.16 numpy-1.24.4 pandas-2.0.3 pathos-0.3.2 pillow-10.4.0 pkgutil-resolve-name-1.3.10 pox-0.3.4 ppft-1.7.6.8 protobuf-4.25.4 pytz-2024.1 referencing-0.35.1 requests-2.32.3 rpds-py-0.20.0 s3transfer-0.10.2 sagemaker-2.227.0 schema-0.7.7 smdebug-rulesconfig-1.0.1 tblib-3.0.0 tqdm-4.66.5 tzdata-2024.1 urllib3-1.26.19\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U sagemaker pillow huggingface-hub conda-pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44c48876",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/davidsanfelix/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidsanfelix/Desktop/Draft AI/triton-mme/.conda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import time\n",
    "import json\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "\n",
    "from utils import download_model\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# variables\n",
    "s3_client = boto3.client(\"s3\")\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "# sagemaker variables\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session())\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"stable-diffusion-mme\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda567c-afaa-4a7e-a947-98f1b15cb358",
   "metadata": {},
   "source": [
    "### Part 2 - Save pretrained model <a name=\"modelartifact\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d8b41-ffc3-49c6-831f-b70e08746284",
   "metadata": {},
   "source": [
    "The `models` directory contains the inference code and the Triton configuration file for each of the Stable Diffusion models. In addition to these, we also need to download the pretrained model weights and save them to ther respective subdirectory within `models` directory. Once we have these downloaded, we can package the inference code and the model weights into a tarball and upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd50657-e4e9-4532-9763-473d1a0d9d38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 18 files: 100%|██████████| 18/18 [00:00<00:00, 65.81it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/sdxl_controlnet/2/checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m models_local_path \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msd-community/sdxl-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/sdxl_controlnet/1/checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxinsir/controlnet-scribble-sdxl-1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/sdxl_controlnet/2/checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model_local_path \u001b[38;5;129;01min\u001b[39;00m models_local_path\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mdownload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_local_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Draft AI/triton-mme/utils.py:8\u001b[0m, in \u001b[0;36mdownload_model\u001b[0;34m(model_name, local_model_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_model\u001b[39m(model_name, local_model_path):\n\u001b[1;32m      7\u001b[0m     local_model_path \u001b[38;5;241m=\u001b[39m Path(local_model_path)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mlocal_model_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     local_cache_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tmp_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     snapshot_download(\n\u001b[1;32m     11\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m     12\u001b[0m         local_dir_use_symlinks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m         ignore_patterns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     16\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Draft AI/triton-mme/.conda/lib/python3.8/pathlib.py:1288\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_closed()\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1288\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/sdxl_controlnet/2/checkpoint'"
     ]
    }
   ],
   "source": [
    "models_local_path = {\n",
    "    \"sd-community/sdxl-flash\": \"models/sdxl_controlnet/1/checkpoint\",\n",
    "}\n",
    "\n",
    "for model_name, model_local_path in models_local_path.items():\n",
    "    download_model(model_name, model_local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d58e0e",
   "metadata": {},
   "source": [
    "### Part 3 - Packaging a conda environment, extending Sagemaker Triton container <a name=\"condaenv\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973a7d2",
   "metadata": {},
   "source": [
    "When using the Triton Python backend (which our Stable Diffusion model will run on), you can include your own environment and dependencies. The recommended way to do this is to use [conda pack](https://conda.github.io/conda-pack/) to generate a conda environment archive in `tar.gz` format, and point to it in the `config.pbtxt` file of the models that should use it, adding the snippet: \n",
    "\n",
    "```\n",
    "parameters: {\n",
    "  key: \"EXECUTION_ENV_PATH\",\n",
    "  value: {string_value: \"path_to_your_env.tar.gz\"}\n",
    "}\n",
    "\n",
    "```\n",
    "You can use a different environment per model, or the same for all models (read more on this [here](https://github.com/triton-inference-server/python_backend#creating-custom-execution-environments)). Since the all of the models that we'll be deploying have the same set of environment requirements, we will create a single conda environment and will use a Python backend to copy that environment into a location where it can be accessed by all models.\n",
    "\n",
    "> ⚠ **Warning**: The approach for a creating a shared conda environment highlighted here is limited to a single instance deployment only. In the event of auto-scaling, there is no guarantee that the new instance will have the conda environment configured. Since the conda environment for hosting Stable Diffusion models is quite large  the recommended approach for production deployments is to create shared environment by extending the Triton Inference Image.  \n",
    "\n",
    "Let's start by creating the conda environment with the necessary dependencies; running these cells will output a `sd_env.tar.gz` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f523d03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile environment.yml\n",
    "name: mme_env\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pip\n",
    "  - pip:\n",
    "      - numpy\n",
    "      - torch --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "      - accelerate\n",
    "      - transformers\n",
    "      - diffusers\n",
    "      - xformers\n",
    "      - conda-pack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b719d4ec-19a1-46cb-a887-9ccd72771e91",
   "metadata": {},
   "source": [
    "Now we can create the environment using the above environment yaml spec\n",
    "\n",
    "🛈 It could take up to 5 min to create the conda environment. Make sure you are running this notebook in an `ml.m5.large` instance or above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147debc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!conda env create -f environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3406a9fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!conda pack -n mme_env -o models/setup_conda/sd_env.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89830242-247e-458a-b2c2-6637c6845872",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 4 - Deploy endpoint <a name=\"deploy\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551fa2a6-3562-49aa-88bc-e35c2cf75c79",
   "metadata": {},
   "source": [
    "Now, we get the correct URI for the SageMaker Triton container image. Check out all the available Deep Learning Container images that AWS maintains [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9b707c2-f78e-452a-9e99-4860232bd76b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "triton_framework = \"sagemaker-tritonserver\"\n",
    "region=\"eu-west-3\"\n",
    "version=\"23.12\"\n",
    "instance_type=\"ml.g4dn.xlarge\"\n",
    "\n",
    "mme_triton_image_uri = image_uris.retrieve(framework=triton_framework, region=region, version=version, instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c7a738",
   "metadata": {},
   "source": [
    "The next step is to package the model subdirectories and weights into individual tarballs and upload them to S3. This process can take about 10 to 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f34cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_root_path = Path(\"./models\")\n",
    "model_dirs = list(model_root_path.glob(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db38cac8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_upload_paths = {}\n",
    "for model_path in model_dirs:\n",
    "    model_name = model_path.name\n",
    "    tar_name = model_path.name + \".tar.gz\"\n",
    "    !tar -C $model_root_path -czvf $tar_name $model_name\n",
    "    model_upload_paths[model_name] = sagemaker_session.upload_data(path=tar_name, bucket=bucket, key_prefix=prefix)\n",
    "    !rm $tar_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c4844-d19c-4e4b-9634-63df5cd41464",
   "metadata": {},
   "source": [
    "We are now ready to configure and deploy the multi-model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e328daa-3d5c-4c66-a6cc-7c80c4274dba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data_url = f\"s3://{bucket}/{prefix}/\"  # s3 location where models are stored\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": model_data_url,\n",
    "    \"Mode\": \"MultiModel\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbcc545-a421-438c-b01f-e6420a504a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb4c11a-4f8a-409e-832e-f27b8be07bb7",
   "metadata": {},
   "source": [
    "Create a SageMaker endpoint configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c9c1a6-92de-44d6-8b0b-7c6f388da957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "instance_type = \"ml.g5.xlarge\"\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09a802-706e-49f4-ae36-cb4e32960b6f",
   "metadata": {},
   "source": [
    "Create the endpoint, and wait for it to transition to `InService` state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39066acd-e31e-433d-9200-65f7bb3b6975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfee5c2-049d-4c96-89ea-725f99003bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(30)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff32048-bf34-4b27-abe7-580929bdbe39",
   "metadata": {},
   "source": [
    "### Query models <a name=\"query\"></a>\n",
    "The endpoint is now deployed and we can query the individual models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be74afb-504c-48ed-b0fc-7177774c1e03",
   "metadata": {},
   "source": [
    "Prior to invoking any of the Stable Diffusion Models, we first invoke the `setup_conda` which will copy the conda environment into a directory that can be shared with all the other models. Refer to the [model.py](./models/setup_conda/1/model.py) file in the `models/setup_conda/1` directory for more details on the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d30696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# invoke the setup_conda model to create the shared conda environment\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"TEXT\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"hello\"],  # dummy data not used by the model\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=\"setup_conda.tar.gz\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093b1151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper functions to encode and decode images\n",
    "def encode_image(image):\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"JPEG\")\n",
    "    img_str = base64.b64encode(buffer.getvalue())\n",
    "\n",
    "    return img_str\n",
    "\n",
    "\n",
    "def decode_image(img):\n",
    "    buff = BytesIO(base64.b64decode(img.encode(\"utf8\")))\n",
    "    image = Image.open(buff)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ca25e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = dict(\n",
    "    prompt=\"Infinity pool on top of a high rise overlooking Central Park\",\n",
    "    negative_prompt=\"blur, signature, low detail, low quality\",\n",
    "    gen_args=json.dumps(dict(num_inference_steps=50, guidance_scale=8)),\n",
    ")\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": name, \"shape\": [1, 1], \"datatype\": \"BYTES\", \"data\": [data]}\n",
    "        for name, data in inputs.items()\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=\"sd_base.tar.gz\",\n",
    ")\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))[\"outputs\"]\n",
    "original_image = decode_image(output[0][\"data\"][0])\n",
    "original_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b76a6-4914-4b5d-af27-fffa7198e4be",
   "metadata": {},
   "source": [
    "Let's take the output from the Standard Model and modify it using the depth model.\n",
    "\n",
    "We can use the same model to change the style of the original image into an oil panting or change the setting from New York City Central Park to the Yellowstone National Park while preserving the orientation of the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71af00d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_image = encode_image(original_image).decode(\"utf8\")\n",
    "\n",
    "inputs = dict(\n",
    "    prompt=\"highly detailed oil painting of an inifinity pool overlooking central park\",\n",
    "    image=input_image,\n",
    "    gen_args=json.dumps(dict(num_inference_steps=50, strength=0.8)),\n",
    ")\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": name, \"shape\": [1, 1], \"datatype\": \"BYTES\", \"data\": [data]}\n",
    "        for name, data in inputs.items()\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=\"sd_depth.tar.gz\",\n",
    ")\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))[\"outputs\"]\n",
    "oil_painting = decode_image(output[0][\"data\"][0])\n",
    "\n",
    "\n",
    "inputs = dict(\n",
    "    prompt=\"Infinity pool perched on a cliff overlooking Yellowstone National Park \",\n",
    "    image=input_image,\n",
    "    gen_args=json.dumps(dict(num_inference_steps=50, strength=0.8)),\n",
    ")\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": name, \"shape\": [1, 1], \"datatype\": \"BYTES\", \"data\": [data]}\n",
    "        for name, data in inputs.items()\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=\"sd_depth.tar.gz\",\n",
    ")\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))[\"outputs\"]\n",
    "rocky_mountains = decode_image(output[0][\"data\"][0])\n",
    "\n",
    "\n",
    "print(\"Original image\")\n",
    "display(original_image)\n",
    "\n",
    "print(\"Oil painting\")\n",
    "display(oil_painting)\n",
    "\n",
    "print(\"Yellowstone\")\n",
    "display(rocky_mountains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2aead9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_image = Image.open(\"sample_images/bertrand-gabioud.png\")\n",
    "\n",
    "image = encode_image(source_image).decode(\"utf8\")\n",
    "mask_image = encode_image(Image.open(\"sample_images/bertrand-gabioud-mask.png\")).decode(\"utf8\")\n",
    "inputs = dict(\n",
    "    prompt=\"building, facade, paint, windows\",\n",
    "    image=image,\n",
    "    mask_image=mask_image,\n",
    "    negative_prompt=\"tree, obstruction, sky, clouds\",\n",
    "    gen_args=json.dumps(dict(num_inference_steps=50, guidance_scale=10)),\n",
    ")\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": name, \"shape\": [1, 1], \"datatype\": \"BYTES\", \"data\": [data]}\n",
    "        for name, data in inputs.items()\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=\"sd_inpaint.tar.gz\",\n",
    ")\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))[\"outputs\"]\n",
    "print(\"source image\")\n",
    "display(source_image)\n",
    "\n",
    "print(\"filled image\")\n",
    "display(decode_image(output[0][\"data\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04832589-d909-460a-a038-9e6f8069f6a3",
   "metadata": {},
   "source": [
    "For the final example we will downsize our original output image from 512x512 to 128x128. We will then use the upscaling model to upscale the image back to its original 512 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef41b693-5344-4ebc-a29b-1b931446a85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "low_res_image = original_image.resize((128, 128))\n",
    "inputs = dict(\n",
    "    prompt=\"Infinity pool on top of a high rise overlooking Central Park\",\n",
    "    image=encode_image(low_res_image).decode(\"utf8\"),\n",
    ")\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": name, \"shape\": [1, 1], \"datatype\": \"BYTES\", \"data\": [data]}\n",
    "        for name, data in inputs.items()\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=\"sd_upscale.tar.gz\",\n",
    ")\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))[\"outputs\"]\n",
    "upscaled_image = decode_image(output[0][\"data\"][0])\n",
    "\n",
    "print(\"Low res image\")\n",
    "display(low_res_image.resize((512, 512)))\n",
    "\n",
    "print(\"Upscaled image\")\n",
    "display(upscaled_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7824e",
   "metadata": {},
   "source": [
    "## Clean up <a name=\"query\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2688904b-426a-4d3b-8e7a-30701f0f752a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=sm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6ef61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#delete models in respective paths\n",
    "for model_name, model_local_path in models_local_path.items():\n",
    "    !rm -rf $model_local_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8f70db-8c31-4a94-a703-2b67c539c0ae",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference|generativeai|llm-workshop|lab2-stable-diffusion|option3-triton-mme|sm-triton-python-stablediff.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference|generativeai|llm-workshop|lab2-stable-diffusion|option3-triton-mme|sm-triton-python-stablediff.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference|generativeai|llm-workshop|lab2-stable-diffusion|option3-triton-mme|sm-triton-python-stablediff.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference|generativeai|llm-workshop|lab2-stable-diffusion|option3-triton-mme|sm-triton-python-stablediff.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference|generativeai|llm-workshop|lab2-stable-diffusion|option3-triton-mme|sm-triton-python-stablediff.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference|generativeai|llm-workshop|lab2-stable-diffusion|option3-triton-mme|sm-triton-python-stablediff.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference|generativeai|llm-workshop|lab2-stable-diffusion|option3-triton-mme|sm-triton-python-stablediff.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference|generativeai|llm-workshop|lab2-stable-diffusion|option3-triton-mme|sm-triton-python-stablediff.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference|generativeai|llm-workshop|lab2-stable-diffusion|option3-triton-mme|sm-triton-python-stablediff.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference|generativeai|llm-workshop|lab2-stable-diffusion|option3-triton-mme|sm-triton-python-stablediff.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference|generativeai|llm-workshop|lab2-stable-diffusion|option3-triton-mme|sm-triton-python-stablediff.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference|generativeai|llm-workshop|lab2-stable-diffusion|option3-triton-mme|sm-triton-python-stablediff.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference|generativeai|llm-workshop|lab2-stable-diffusion|option3-triton-mme|sm-triton-python-stablediff.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference|generativeai|llm-workshop|lab2-stable-diffusion|option3-triton-mme|sm-triton-python-stablediff.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference|generativeai|llm-workshop|lab2-stable-diffusion|option3-triton-mme|sm-triton-python-stablediff.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
